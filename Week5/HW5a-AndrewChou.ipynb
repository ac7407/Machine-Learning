{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6129c9d5",
   "metadata": {},
   "source": [
    "# Train and fine-tune a decision tree for the moons dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696304fa",
   "metadata": {},
   "source": [
    "# Step 1: Use make_moons(n_samples=10000, noise=0.4) to generate a moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "734dc1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.24.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (0.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d2f0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8ffa04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate moons dataset\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e474cb",
   "metadata": {},
   "source": [
    "# Step 2: Use train_test_split() to split the dataset into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35926042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length:  8000\n",
      "Test set length:  2000\n"
     ]
    }
   ],
   "source": [
    "# split dataset into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# print length of training and test set\n",
    "print(\"Training set length: \", len(X_train))\n",
    "print(\"Test set length: \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbade51",
   "metadata": {},
   "source": [
    "# Step 3: Use grid search with cross-validation (with the help of the GridSearchCV class) to find good hyperparameter values for a DecisionTreeClassifier. Hint: try various values for max_leaf_nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43b1ecd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 294 candidates, totalling 882 fits\n",
      "Best hyperparameters:  {'max_leaf_nodes': 17, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "# use GridSearchCV to find the best hyperparameters for a DecisionTreeClassifier\n",
    "# try various values for max_leaf_nodes\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4]}\n",
    "]\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(tree_clf, param_grid, cv=3, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print the best hyperparameters\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e01639",
   "metadata": {},
   "source": [
    "# Step 4: Train it on the full training set using these hyperparameters, and measure your model’s performance on the test set. You should get roughly 85% to 87% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c960cad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  0.8695\n"
     ]
    }
   ],
   "source": [
    "# train on the entire training set with the best hyperparameters\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "print(\"Test set accuracy: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e8a02",
   "metadata": {},
   "source": [
    "# Grow a forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8d921a",
   "metadata": {},
   "source": [
    "# Step 1: Continuing the previous exercise, generate 1,000 subsets of the training set, each containing 100 instances selected randomly. Hint: you can use Scikit-Learn’s ShuffleSplit class for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0208d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ShuffleSplit to create 1000 subsets of the training set, each containing 100 instances chosen randomly\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "n_trees = 1000\n",
    "n_instances = 100\n",
    "\n",
    "mini_sets = []\n",
    "\n",
    "rs = ShuffleSplit(n_splits=n_trees, test_size=len(X_train) - n_instances, random_state=42)\n",
    "for mini_train_index, mini_test_index in rs.split(X_train):\n",
    "    X_mini_train = X_train[mini_train_index]\n",
    "    y_mini_train = y_train[mini_train_index]\n",
    "    mini_sets.append((X_mini_train, y_mini_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210d81b",
   "metadata": {},
   "source": [
    "# Step 2: Train one decision tree on each subset, using the best hyperparameter values found in the previous exercise. Evaluate these 1,000 decision trees on the test set. Since they were trained on smaller sets, these decision trees will likely perform worse than the first decision tree, achieving only about 80% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4c0b203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of 1000 decision trees:  0.805471\n"
     ]
    }
   ],
   "source": [
    "# train one decision tree on each subset\n",
    "# use best hyperparameters from previous step\n",
    "# evaluate 1000 decision trees on the test set\n",
    "from sklearn.base import clone\n",
    "\n",
    "forest = [clone(final_model) for _ in range(n_trees)]\n",
    "\n",
    "accuracy_scores = []\n",
    "\n",
    "for tree, (X_mini_train, y_mini_train) in zip(forest, mini_sets):\n",
    "    tree.fit(X_mini_train, y_mini_train)\n",
    "\n",
    "    y_pred = tree.predict(X_test)\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# print the mean accuracy of the 1000 decision trees\n",
    "print(\"Mean accuracy of 1000 decision trees: \", np.mean(accuracy_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c80b747",
   "metadata": {},
   "source": [
    "# Step 3: Now comes the magic. For each test set instance, generate the predictions of the 1,000 decision trees, and keep only the most frequent prediction (you can use SciPy’s mode() function for this). This approach gives you majority-vote predictions over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b2174ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_315/725574019.py:10: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  y_pred_majority_votes, n_votes = mode(Y_pred, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# generate predictions for 1000 decision trees\n",
    "# use scipy's mode() function to find the most frequent prediction\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n",
    "Y_pred = np.empty([n_trees, len(X_test)], dtype=np.uint8)\n",
    "for tree_index, tree in enumerate(forest):\n",
    "    Y_pred[tree_index] = tree.predict(X_test)\n",
    "    \n",
    "y_pred_majority_votes, n_votes = mode(Y_pred, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e67126",
   "metadata": {},
   "source": [
    "# Step 4: Evaluate these predictions on the test set: you should obtain a slightly higher accuracy than your first model (about 0.5 to 1.5% higher). Congratulations, you have trained a random forest classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef9001b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of majority voting:  0.872\n"
     ]
    }
   ],
   "source": [
    "# evaluate these predictions on the test set\n",
    "print(\"Accuracy of majority voting: \", accuracy_score(y_test, y_pred_majority_votes.reshape([-1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
