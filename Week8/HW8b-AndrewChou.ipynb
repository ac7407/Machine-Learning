{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With MNIST data set, to randomly use 10000 images as the training set, 5000 images as the validation set and use the test set as provided, and explore how does the network architecture [number of hidden layers, number of dimensions per hidden layer, activation functions], Batch normalization, optimization algorithms, learning rate and batch size would affect the training accuracy and validation accuracy. Finally, for the best model that achieved in validation accuracy, you further train it with the whole training set, what would be your training accuracy, validation accuracy and test accuracy.  Please summarize what you have learned from this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/ac7407/.local/lib/python3.8/site-packages (2.0.0)\n",
      "Requirement already satisfied: torchvision in /home/ac7407/.local/lib/python3.8/site-packages (0.15.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ac7407/.local/lib/python3.8/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ac7407/.local/lib/python3.8/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ac7407/.local/lib/python3.8/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: networkx in /home/ac7407/.local/lib/python3.8/site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ac7407/.local/lib/python3.8/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ac7407/.local/lib/python3.8/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ac7407/.local/lib/python3.8/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ac7407/.local/lib/python3.8/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: jinja2 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from torch) (2.11.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ac7407/.local/lib/python3.8/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ac7407/.local/lib/python3.8/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ac7407/.local/lib/python3.8/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ac7407/.local/lib/python3.8/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ac7407/.local/lib/python3.8/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ac7407/.local/lib/python3.8/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: sympy in /home/ac7407/.local/lib/python3.8/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: filelock in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from torch) (3.0.12)\n",
      "Requirement already satisfied: numpy in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages/numpy-1.19.2-py3.8-linux-x86_64.egg (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from torchvision) (8.0.1)\n",
      "Requirement already satisfied: requests in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from torchvision) (2.24.0)\n",
      "Requirement already satisfied: setuptools in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (49.2.1)\n",
      "Requirement already satisfied: wheel in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (0.35.1)\n",
      "Requirement already satisfied: lit in /home/ac7407/.local/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (16.0.0)\n",
      "Requirement already satisfied: cmake in /home/ac7407/.local/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (3.26.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ac7407/.local/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests->torchvision) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests->torchvision) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests->torchvision) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests->torchvision) (3.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/ac7407/.local/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ac7407/.local/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages/numpy-1.19.2-py3.8-linux-x86_64.egg (from scikit-learn) (1.19.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages/scipy-1.5.2-py3.8-linux-x86_64.egg (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ac7407/.local/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MNIST dataset using torchvision module\n",
    "# split the dataset into training (10000 images), validation (5000 images), and test sets\n",
    "image_path = './'\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "])\n",
    "\n",
    "mnist_dataset = torchvision.datasets.MNIST(\n",
    "        root=image_path, train=True,\n",
    "        transform=transform, download=False\n",
    ")\n",
    "\n",
    "train_size = 10000\n",
    "val_size = 5000\n",
    "test_size = len(mnist_dataset) - train_size - val_size\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(mnist_dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  10000\n",
      "Validation set size:  5000\n",
      "Test set size:  45000\n"
     ]
    }
   ],
   "source": [
    "# verify length of each dataset\n",
    "print('Training set size: ', len(train_ds))\n",
    "print('Validation set size: ', len(val_ds))\n",
    "print('Test set size: ', len(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore number of hidden layers, number of dimensions per hidden layer, activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct NN model \n",
    "hidden_units = [32, 16]\n",
    "image_size = mnist_dataset[0][0].shape\n",
    "input_size = image_size[0] * image_size[1] * image_size[2]\n",
    "\n",
    "all_layers = [nn.Flatten()]\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "\n",
    "all_layers.append(nn.Linear(hidden_units[-1], 10))\n",
    "model = nn.Sequential(*all_layers)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# define Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training function\n",
    "def train(model, train_dl, loss_fn, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_dl:\n",
    "            images, labels = batch\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.3033\n",
      "Epoch 2/10, Loss: 0.1942\n",
      "Epoch 3/10, Loss: 0.1593\n",
      "Epoch 4/10, Loss: 0.1285\n",
      "Epoch 5/10, Loss: 0.1064\n",
      "Epoch 6/10, Loss: 0.0798\n",
      "Epoch 7/10, Loss: 0.0608\n",
      "Epoch 8/10, Loss: 0.0468\n",
      "Epoch 9/10, Loss: 0.0392\n",
      "Epoch 10/10, Loss: 0.0316\n"
     ]
    }
   ],
   "source": [
    "# function call\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_ds, batch_size)\n",
    "val_dl = DataLoader(val_ds, batch_size)\n",
    "train(model, train_dl, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.28%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test set\n",
    "def evaluate(model, test_dl):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in test_dl:\n",
    "        images, labels = batch\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {correct/total*100:.2f}%\")\n",
    "\n",
    "evaluate(model, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweak parameters and retrain the model\n",
    "# parameters: number of hidden layers, number of dimensions per hidden layer, activation functions\n",
    "\n",
    "# construct NN model -- Trial 2\n",
    "hidden_units = [32, 16]\n",
    "image_size = mnist_dataset[0][0].shape\n",
    "input_size = image_size[0] * image_size[1] * image_size[2]\n",
    "\n",
    "all_layers = [nn.Flatten()]\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "\n",
    "all_layers.append(nn.Linear(hidden_units[-1], 10))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross-entropy loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# use SGD optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.3202\n",
      "Epoch 2/10, Loss: 2.2983\n",
      "Epoch 3/10, Loss: 2.2749\n",
      "Epoch 4/10, Loss: 2.2503\n",
      "Epoch 5/10, Loss: 2.2227\n",
      "Epoch 6/10, Loss: 2.1915\n",
      "Epoch 7/10, Loss: 2.1567\n",
      "Epoch 8/10, Loss: 2.1177\n",
      "Epoch 9/10, Loss: 2.0743\n",
      "Epoch 10/10, Loss: 2.0264\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_ds, batch_size)\n",
    "val_dl = DataLoader(val_ds, batch_size)\n",
    "train(model, train_dl, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 37.88%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test set\n",
    "evaluate(model, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct NN model -- Trial 3\n",
    "hidden_units = [32, 16]\n",
    "image_size = mnist_dataset[0][0].shape\n",
    "input_size = image_size[0] * image_size[1] * image_size[2]\n",
    "\n",
    "all_layers = [nn.Flatten()]\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "\n",
    "all_layers.append(nn.Linear(hidden_units[-1], 10))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross-entropy loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# use RMSprop optimizer\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5309\n",
      "Epoch 2/10, Loss: 0.3773\n",
      "Epoch 3/10, Loss: 0.2972\n",
      "Epoch 4/10, Loss: 0.2408\n",
      "Epoch 5/10, Loss: 0.1766\n",
      "Epoch 6/10, Loss: 0.1224\n",
      "Epoch 7/10, Loss: 0.0871\n",
      "Epoch 8/10, Loss: 0.0648\n",
      "Epoch 9/10, Loss: 0.0497\n",
      "Epoch 10/10, Loss: 0.0445\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_ds, batch_size)\n",
    "val_dl = DataLoader(val_ds, batch_size)\n",
    "train(model, train_dl, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.74%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test set\n",
    "evaluate(model, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Between using SGD, Adam, and RMSProp, the best optimizer for this classification problem will be the RMSProp optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (2): Softmax(dim=None)\n",
       "  (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (4): Softmax(dim=None)\n",
       "  (5): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (6): Softmax(dim=None)\n",
       "  (7): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (8): Softmax(dim=None)\n",
       "  (9): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (10): Softmax(dim=None)\n",
       "  (11): Linear(in_features=2, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct NN model -- Trial 4 (5 hidden layers, 32, 16, 8, 4, 2 dimensions per layer, Softmax activation function)\n",
    "hidden_units = [32, 16, 8, 4, 2]\n",
    "image_size = mnist_dataset[0][0].shape\n",
    "input_size = image_size[0] * image_size[1] * image_size[2]\n",
    "\n",
    "all_layers = [nn.Flatten()]\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.Softmax())\n",
    "    input_size = hidden_unit\n",
    "\n",
    "all_layers.append(nn.Linear(hidden_units[-1], 10))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross-entropy loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# use RMSProp optimizer but increase learning rate\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac7407/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.8222\n",
      "Epoch 2/10, Loss: 1.7595\n",
      "Epoch 3/10, Loss: 1.6966\n",
      "Epoch 4/10, Loss: 1.6492\n",
      "Epoch 5/10, Loss: 1.8485\n",
      "Epoch 6/10, Loss: 1.6031\n",
      "Epoch 7/10, Loss: 1.5109\n",
      "Epoch 8/10, Loss: 1.4111\n",
      "Epoch 9/10, Loss: 1.3475\n",
      "Epoch 10/10, Loss: 1.3208\n"
     ]
    }
   ],
   "source": [
    "# train the model with increased batch size \n",
    "batch_size = 64\n",
    "train_dl = DataLoader(train_ds, batch_size)\n",
    "val_dl = DataLoader(val_ds, batch_size)\n",
    "train(model, train_dl, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 37.50%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test set\n",
    "evaluate(model, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (6): ReLU()\n",
       "  (7): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (8): ReLU()\n",
       "  (9): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (10): ReLU()\n",
       "  (11): Linear(in_features=2, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct NN model -- Trial 5 (5 hidden layers, 32, 16, 8, 4, 2 dimensions per layer, ReLU activation function)\n",
    "hidden_units = [32, 16, 8, 4, 2]\n",
    "image_size = mnist_dataset[0][0].shape\n",
    "input_size = image_size[0] * image_size[1] * image_size[2]\n",
    "\n",
    "all_layers = [nn.Flatten()]\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "\n",
    "all_layers.append(nn.Linear(hidden_units[-1], 10))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross-entropy loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# use RMSProp optimizer\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.0656\n",
      "Epoch 2/10, Loss: 1.9504\n",
      "Epoch 3/10, Loss: 1.8738\n",
      "Epoch 4/10, Loss: 1.8231\n",
      "Epoch 5/10, Loss: 1.7858\n",
      "Epoch 6/10, Loss: 1.7349\n",
      "Epoch 7/10, Loss: 1.6579\n",
      "Epoch 8/10, Loss: 1.6046\n",
      "Epoch 9/10, Loss: 1.5541\n",
      "Epoch 10/10, Loss: 1.5313\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_ds, batch_size)\n",
    "val_dl = DataLoader(val_ds, batch_size)\n",
    "train(model, train_dl, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 45.02%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test set\n",
    "evaluate(model, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on trials 4 and 5, it appears using a lower batch size (32) as opposed to a higher one (64) yields better accuracy. Additionally, with more hidden layers the accuracy worsens. Also, using the Softmax activation function yields a lower accuracy result than using ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct NN model -- Trial 6\n",
    "hidden_units = [32, 16]\n",
    "image_size = mnist_dataset[0][0].shape\n",
    "input_size = image_size[0] * image_size[1] * image_size[2]\n",
    "\n",
    "all_layers = [nn.Flatten()]\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "\n",
    "all_layers.append(nn.Linear(hidden_units[-1], 10))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross-entropy loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# use RMSProp optimizer\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.4404\n",
      "Epoch 2/10, Loss: 0.2819\n",
      "Epoch 3/10, Loss: 0.2163\n",
      "Epoch 4/10, Loss: 0.1818\n",
      "Epoch 5/10, Loss: 0.1559\n",
      "Epoch 6/10, Loss: 0.1324\n",
      "Epoch 7/10, Loss: 0.1095\n",
      "Epoch 8/10, Loss: 0.0950\n",
      "Epoch 9/10, Loss: 0.0815\n",
      "Epoch 10/10, Loss: 0.0692\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_ds, batch_size)\n",
    "val_dl = DataLoader(val_ds, batch_size)\n",
    "train(model, train_dl, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.44%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test set\n",
    "evaluate(model, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct NN model -- Trial 7\n",
    "hidden_units = [32, 16]\n",
    "image_size = mnist_dataset[0][0].shape\n",
    "input_size = image_size[0] * image_size[1] * image_size[2]\n",
    "\n",
    "all_layers = [nn.Flatten()]\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "\n",
    "all_layers.append(nn.Linear(hidden_units[-1], 10))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross-entropy loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# use RMSProp optimizer\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.6943\n",
      "Epoch 2/10, Loss: 1.2404\n",
      "Epoch 3/10, Loss: 0.9222\n",
      "Epoch 4/10, Loss: 0.7292\n",
      "Epoch 5/10, Loss: 0.6094\n",
      "Epoch 6/10, Loss: 0.5321\n",
      "Epoch 7/10, Loss: 0.4776\n",
      "Epoch 8/10, Loss: 0.4361\n",
      "Epoch 9/10, Loss: 0.4015\n",
      "Epoch 10/10, Loss: 0.3714\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_ds, batch_size)\n",
    "val_dl = DataLoader(val_ds, batch_size)\n",
    "train(model, train_dl, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.54%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test set\n",
    "evaluate(model, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decreasing the learning rate to 1e-4 is worse than if the learning rate is 1e-3, therefore the best learning rate to train this classification problem is 1e-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The best neural network architecture found is to build two hidden layers with dimensions 32 and 16. Use ReLU as an activation function, use cross-entropy loss function, use RMSProp as the optimizer with a learning rate of 1e-3, and finally to set the batch size to 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
